# Pose_Interactions_UseTech

## Описание задачи

Разработка системы анализа видео с камер наблюдения, которая:
- обнаруживает людей на каждом кадре видеопотока,
- отслеживает их перемещение по кадрам с помощью трекинга,
- определяет позы людей,
- определяет взаимодействия между людьми (рукопожатие и объятия).

## Структура проекта
```
project/
├── main.py                  # Главный файл запуска пайплайна
├── detector.py              # Обнаружение людей
├── pose_estimator.py        # Детекция ключевых точек
├── tracker_module.py        # Трекинг людей
├── draw_utils.py            # Отрисовка keypoints и skeleton
├── behavior_analyzer.py     # Логика анализа взаимодействий
├── config.py                # Конфигурационные параметры
├── track_data.json          # Итоговый JSON-файл с данными
└── requirements.txt         # Зависимости
```

## Установка и запуск

1. Установите зависимости:

```bash
pip install -r requirements.txt
```
(рекомендуется установить `torch` вручную для правильной версии CUDA)

2. Подготовьте входные файлы:

- `shop_test.mp4` — видео для обработки
- `yolo11x.pt` — обученная модель YOLOv11 для детекции
- `yolo11x-pose.pt` — обученная модель YOLOv11 Pose для поз

3. Запуск:

```bash
python main.py
```

## Пайплайн обработки видео

### 1. Загрузка и инициализация (`config.py`)

- Загрузка входного видео `shop_test.mp4` с помощью OpenCV (`cv2.VideoCapture`).
- Получение:
  - размеров кадра (`width`, `height`)
  - частоты кадров (`fps`)
- Подготовка объекта записи для выходного видео `yolo11test_unskelet.mp4`.
- Задание конфигураций:
  - `conf_threshold = 0.5` — порог уверенности YOLO-детектора
  - `keypoint_threshpoint = 0.3` — минимальная уверенность ключевой точки
  (данные конфигурации оптимизированы для качества видео с камер наблюдения)

### 2. Детекция людей на кадре (`detector.py`)

- Загружает модель `yolo11x.pt`.
- Функция `detect_people(frame)`:
  - Принимает кадр.
  - Возвращает bounding boxes для объектов класса `"person"` с фильтрацией по уверенности (`conf > 0.5`) и площади bbox (1000 < area < 50000).

### 3. Трекинг объектов (`tracker_module.py`)

- Создаёт экземпляр трекера `DeepSort` с параметром `max_age=30`.
- Функция `update_tracks(detections, frame)`:
  - Принимает bbox'ы и кадр.
  - Возвращает список активных треков с `track_id`.

### 4. Вырезка ROI и определение поз (`pose_estimator.py`)

- Загружает модель `yolo11x-pose.pt`.
- Функция `get_pose_from_roi(roi)`:
  - Принимает вырезанную область (ROI, минимум 30x30 пикселей).
  - Возвращает список из 17 ключевых точек (`keypoints`) в формате COCO, фильтруя по уверенности (`keypoint_threshpoint = 0.3`).
  - Невалидные точки помечаются как `[-1, -1]`.

### 5. Отрисовка позы и скелета (`draw_utils.py`)

- Определяет связи между ключевыми точками (скелет) для верхней и нижней части тела (например, плечи-запястья, бёдра-лодыжки).
- Функция `draw_skeleton(frame, keypoints, offset_x, offset_y)`:
  - Рисует чёрные точки для валидных keypoints и чёрные линии для связей.

### 6. Анализ взаимодействий (`behavior_analyzer.py`)

- Анализирует взаимодействия (рукопожатие, объятия) на основе:
  - Расстояния между носами (`keypoints[0]`, порог < 0.2 * frame_width).
  - Ориентации лиц (cos_angle > 0.8 между векторами от середины глаз к носу).
  - Расстояния между запястьями (`keypoints[9, 10]`, < 0.03 * frame_width для рукопожатия) или плечами/носами (< 0.1 * frame_width для объятий).
  - Временной стабильности (подтверждение в течение `MIN_INTERACTION_FRAMES = 3`).
- Использует временные ряды (`trajectory_history`) для проверки неподвижности.
- Фильтрует пары, исключая ложные срабатывания через стабильность пар и историю взаимодействий (`pair_history`).

### 7. Сбор всех модулей (`main.py`)

- **Инициализация**:
  - Открывает видео (`cv2.VideoCapture`).
  - Извлекает параметры: ширину, высоту, fps.
  - Создаёт объект `cv2.VideoWriter` для сохранения выходного видео.
- **Цикл обработки кадров**:
  - Считывает кадр.
  - Вызывает:
    - `detect_people()` для получения bbox людей.
    - `update_tracks()` для отслеживания ID.
    - `get_pose_from_roi()` для извлечения ключевых точек.
    - `analyze_behavior()` для определения взаимодействий.
    - `draw_skeleton()` и отрисовку bbox (зелёные прямоугольники) и ID.
  - Отрисовывает линии между центрами bbox пар для взаимодействий (`handshake`: пурпурные, `hug`: жёлтые).
- **Формирует JSON-структуру**:
  - Для каждого кадра:
    - Сохраняет `timestamp` (время кадра в секундах).
    - Собирает список всех отслеживаемых людей с их:
      - ID
      - Координатами bbox
      - Keypoints (17 штук)
      - Взаимодействиями (`handshake`, `hug`)
  - Данные аккумулируются в словарь `track_data`.
  - Пример структуры JSON:
    ```json
    {
      "frame_00025": {
        "timestamp": 1.05,
        "people": [
          {
            "id": 3,
            "bbox": [340, 150, 420, 360],
            "keypoints": [[351, 160], [-1, -1], ..., [-1, -1]],
            "flags": {}
          }
        ],
        "interactions": [
          {
            "pair": [3, 4],
            "type": "handshake"
          }
        ]
      }
    }
    ```
- **Сохраняет результаты**:
  - Кадр записывается в выходное видео с аннотациями (`yolo11test_unskelet.mp4`).
  - JSON-файл записывается на диск (`track_data.json`).

## Используемые технологии
- OpenCV
- PyTorch
- YOLOv11 (`yolo11x.pt` для детекции, `yolo11x-pose.pt` для поз)
- DeepSORT (через `deep_sort_realtime`)
- JSON, NumPy


## Автор
**Гарник Матевосян**  
Факультет прикладной математики и информатики  
2025
